<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Abstract - Reward Learning on Policy</title>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        .content {
            max-width: 800px;
            margin: 0 auto;
        }
        .center-text {
            text-align: center;
            font-weight: bold;
            margin-top: 30px;
        }
        .english {
            color: #333;
            font-weight: bold;
            margin-top: 20px;
        }
        .chinese {
            color: #555;
            margin-bottom: 20px;
        }
.mathjax {
    font-size: 1.2em;
    line-height: 1.5;
}

.mathjax sup, .mathjax sub {
    font-size: 0.8em;
}

        a {
            color: #0066cc;
        }
    </style>
</head>
<body>

    <div class="content">
        <p class="center-text">Abstract</p>
        <p class="center-text">摘要</p>

        <p class="english">Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences.</p>
        <p class="chinese">从人类反馈中进行强化学习（RLHF）已成为将大型语言模型（LLMs）与人类偏好对齐的有效方法。</p>

        <p class="english">RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially.</p>
        <p class="chinese">RLHF包含三个步骤：人类偏好收集、奖励学习和策略优化，这些步骤通常是串行执行的。</p>

        <p class="english">Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs’ data distribution.</p>
        <p class="chinese">然而，尽管RLHF很受欢迎，（固定的）奖励模型可能会受到离群数据的影响，因为策略优化不断改变LLMs的数据分布。</p>

        <p class="english">Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.</p>
        <p class="chinese">重复收集最新LLMs的偏好数据可以缓解这个问题，但不幸的是，这会使得系统变得更加复杂且难以优化。</p>

        <p class="english">In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution.</p>
        <p class="chinese">本文提出了基于策略的奖励学习（RLP），这是一种无监督框架，使用策略样本来细化奖励模型，以保持其在数据分布上的一致性。</p>

        <p class="english">Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples.</p>
        <p class="chinese">具体而言，提出了一种无监督的多视图学习方法，用于学习策略样本的鲁棒表示。</p>

        <p class="english">Meanwhile, a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs.</p>
        <p class="chinese">同时，还开发了一种合成偏好生成方法，利用策略输出模拟高质量的偏好数据。</p>

        <p class="english">Extensive experiments on three benchmark datasets show that RLP consistently outperforms the state-of-the-art.</p>
        <p class="chinese">针对三个基准数据集的广泛实验表明，RLP consistently 超越了现有的最先进方法。</p>

        <p class="english">Our code is available at <a href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp" target="_blank">https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp</a>.</p>
        <p class="chinese">我们的代码可在 <a href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp" target="_blank">https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp</a> 找到。</p>
        <p class="english">Figure 1: Comparison of standard RLHF (top) and RLHF with reward learning on policies (bottom). Different from (top), which performs reward learning and policy optimization serially, we iteratively train one of the two models with the help of the other.</p>
<p class="chinese">图 1：标准 RLHF（上）与基于策略奖励学习的 RLHF（下）的对比。不同于（上）采用奖励学习和策略优化串行执行，我们是通过相互辅助，迭代训练两个模型中的一个。</p>

<!-- 嵌入图片 -->
<img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/1-Figure1-1.png" alt="Figure 1: Comparison of standard RLHF and RLHF with reward learning on policies" width="600" />

        
        <p class="center-text">1 Introduction</p>
        <p class="center-text">1 引言</p>
	<p class="english">Large language models (LLMs) (Brown et al., 2020; Bommasani et al., 2021) have shown great promise in following open-ended user instructions (Askell et al., 2021; Ouyang et al., 2022; Longpre et al., 2023).</p>
<p class="chinese">大型语言模型（LLMs）（Brown 等，2020；Bommasani 等，2021）在执行开放式用户指令方面展现出了巨大的潜力（Askell 等，2021；Ouyang 等，2022；Longpre 等，2023）。</p>

<p class="english">These capabilities are largely attributed to the fine-tuning of pretrained LLMs using Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022a), which is a prominent technique to align LLMs with human preferences and greatly enhance their usability and safety (OpenAI, 2023; Anthropic, 2023; Google, 2023).</p>
<p class="chinese">这些能力在很大程度上归因于使用人类反馈强化学习（RLHF）（Christiano 等，2017；Bai 等，2022a）对预训练LLMs进行微调，这是一种将LLMs与人类偏好对齐并大大提高其可用性和安全性的显著技术（OpenAI，2023；Anthropic，2023；Google，2023）。</p>

<p class="english">A typical RLHF procedure is comprised of three interrelated steps: human preference collecting, reward learning, and policy optimization (Figure 1 top).</p>
<p class="chinese">典型的RLHF过程包含三个相互关联的步骤：人类偏好收集、奖励学习和策略优化（见图1顶部）。</p>

<p class="english">The reward learning step fits a reward model to the preference data that elicits evaluations from humans.</p>
<p class="chinese">奖励学习步骤将奖励模型拟合到从人类评估中获得的偏好数据上。</p>

<p class="english">The policy optimization step uses reinforcement learning (RL) to fine-tune a language model to produce outputs assigned high reward.</p>
<p class="chinese">策略优化步骤使用强化学习（RL）对语言模型进行微调，使其生成被赋予高奖励的输出。</p>

<p class="english">In practice, the three key steps of RLHF are often performed serially (Casper et al., 2023).</p>
<p class="chinese">在实践中，RLHF的三个关键步骤通常是串行执行的（Casper 等，2023）。</p>

<p class="english">Since policy optimization shifts the language model’s data distribution during the RL phase, the (fixed) reward model will be inaccurate off-distribution which is trained on offline data (Touvron et al., 2023b).</p>
<p class="chinese">由于策略优化在RL阶段改变了语言模型的数据分布，（固定的）奖励模型会在离群数据上不准确，因为它是基于离线数据训练的（Touvron 等，2023b）。</p>

<p class="english">Hence, reward model accuracy can quickly degrade and in turn degenerate the policy that exploits differences between the inferred and true reward (Gao et al., 2023).</p>
<p class="chinese">因此，奖励模型的准确性会迅速下降，进而导致策略的退化，利用推测奖励与真实奖励之间的差异（Gao 等，2023）。</p>

<p class="english">The above issue can be mitigated by gathering new human preference data from an up-to-date version of policy (Ziegler et al., 2019).</p>
<p class="chinese">上述问题可以通过从最新版本的策略中收集新的人工偏好数据来缓解（Ziegler 等，2019）。</p>

<p class="english">However, the resulting system is significantly more complicated and difficult to optimize, involving iterations of data gathering, reward learning, and RL fine-tuning.</p>
<p class="chinese">然而，这样的系统会变得更加复杂且难以优化，涉及数据收集、奖励学习和RL微调的迭代过程。</p>

<p class="english">Moreover, significant work is required to maintain high data quality over a long time in this setting.</p>
<p class="chinese">此外，在这种环境中，还需要大量工作来保持数据质量的长期高水平。</p>
<p class="english">In this paper, we show how to optimize a reward model against the policy to keep it on-distribution, without repeatedly collecting new human preference data.</p>
<p class="chinese">在本文中，我们展示了如何针对策略优化奖励模型，以保持其数据分布一致，而无需反复收集新的人工偏好数据。</p>

<p class="english">We propose Reward Learning on Policy (RLP), a framework that refines a reward model using policy samples in an unsupervised manner.</p>
<p class="chinese">我们提出了基于策略的奖励学习（RLP），这是一个通过无监督方式使用策略样本来细化奖励模型的框架。</p>

<p class="english">RLP first trains a reward model and a language model policy from scratch with standard RLHF methods, and then retrains the reward model when exposed to the sample distribution of the trained policy.</p>
<p class="chinese">RLP首先使用标准RLHF方法从头开始训练一个奖励模型和语言模型策略，然后在暴露于训练策略的样本分布时重新训练奖励模型。</p>

<p class="english">Finally, RLP retrains the policy on the retrained reward model, which attempts to maintain an accurate reward for the latest policy.</p>
<p class="chinese">最后，RLP在重新训练的奖励模型上重新训练策略，以保持最新策略的准确奖励。</p>

<p class="english">Concretely, RLP uses policy samples to retrain the reward model via two methods: unsupervised multi-view learning (UML) and synthetic preference generation (SPG).</p>
<p class="chinese">具体而言，RLP通过两种方法使用策略样本重新训练奖励模型：无监督多视图学习（UML）和合成偏好生成（SPG）。</p>

<p class="english">RLP-UML constructs two views for an input by generating two responses from the policy (Zhao et al., 2017), then optimizes a multi-view information bottleneck loss (Federici et al., 2020) when fitting the reward model to a dataset of human preferences.</p>
<p class="chinese">RLP-UML通过从策略生成两个响应为输入构建两个视图（Zhao 等，2017），然后在将奖励模型拟合到人类偏好数据集时优化多视图信息瓶颈损失（Federici 等，2020）。</p>

<p class="english">This training objective follows the information bottleneck principle (Tishby et al., 2000) and helps learn robust representations of the policy’s data distribution.</p>
<p class="chinese">这个训练目标遵循信息瓶颈原理（Tishby 等，2000），有助于学习策略数据分布的鲁棒表示。</p>

<p class="english">In addition, RLP-SPG simulates preferences on policy generations to supplement the human preference data.</p>
<p class="chinese">此外，RLP-SPG模拟策略生成的偏好，以补充人工偏好数据。</p>

<p class="english">Rather than producing and scoring two outputs with LLMs as in Reinforcement Learning from AI Feedback (RLAIF) (Bai et al., 2022b; Lee et al., 2023), RLP-SPG generates a set of outputs for an instruction.</p>
<p class="chinese">与人工智能反馈强化学习（RLAIF）（Bai 等，2022b；Lee 等，2023）中使用LLMs生成和评分两个输出不同，RLP-SPG为一个指令生成一组输出。</p>

<p class="english">In this way, RLP-SPG can quantify uncertainty and decide when to trust model predictions via measuring the size of the largest semantic equivalence cluster (Kuhn et al., 2023; Si et al., 2023).</p>
<p class="chinese">通过这种方式，RLP-SPG可以量化不确定性，并通过测量最大语义等价集群的大小来决定何时信任模型预测（Kuhn 等，2023；Si 等，2023）。</p>

<p class="english">Thus, RLP-SPG selectively generates pairwise preferences for instructions with low uncertainty (Lin et al., 2023), where the preferred output is sampled from the largest cluster of the output set and the non-preferred one is sampled from the rest clusters.</p>
<p class="chinese">因此，RLP-SPG为低不确定性的指令选择性地生成成对的偏好（Lin 等，2023），其中偏好的输出从输出集的最大集群中采样，而不偏好的输出则从其他集群中采样。</p>

<p class="english">This sampling scheme also conforms to the self-consistency assumption, i.e., the most consistent output is selected as the final prediction (Wang et al., 2023; Chen et al., 2023).</p>
<p class="chinese">这种采样方案也符合自一致性假设，即选择最一致的输出作为最终预测（Wang 等，2023；Chen 等，2023）。</p>

<p class="english">Our main contributions are as follows:</p>
<p class="chinese">我们的主要贡献如下：</p>

<p class="english">• We propose Reward Learning on Policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution for RLHF.</p>
<p class="chinese">• 我们提出了基于策略的奖励学习（RLP），这是一个无监督框架，使用策略样本来细化奖励模型，以保持其在数据分布上的一致性。</p>

<p class="english">• We optimize a multi-view loss when retraining the reward model to learn representations of the policy’s data distribution. We also simulate preferences with a set of policy outputs, which enables selective generation and high-quality data construction.</p>
<p class="chinese">• 在重新训练奖励模型时，我们优化多视图损失，以学习策略数据分布的表示。我们还通过一组策略输出模拟偏好，从而实现选择性生成和高质量数据构建。</p>

<p class="english">• Our experiments on three standard benchmark datasets show that RLP outperforms existing methods for learning from human feedback, including PPO-based RLHF.</p>
<p class="chinese">• 我们在三个标准基准数据集上的实验表明，RLP优于现有的人工反馈学习方法，包括基于PPO的RLHF。</p>
<p class="center-text">2 Related Work</p>
        <p class="center-text">2 相关工作</p>
<p class="english">Instruction tuning is a procedure to fine-tune pretrained LLMs with instructions and human-written completions (Mishra et al., 2022; Sanh et al., 2022), which increases the usability of LLMs (Chung et al., 2022).</p>
<p class="chinese">指令调优是通过指令和人工编写的完成来微调预训练LLMs的过程（Mishra 等，2022；Sanh 等，2022），它提高了LLMs的可用性（Chung 等，2022）。</p>

<p class="english">Recently, RLHF has emerged as the central method for fine-tuning LLMs based on human preferences and further improves their downstream task performance and alignment with user intent (Christiano et al., 2017).</p>
<p class="chinese">最近，RLHF成为基于人类偏好微调LLMs的核心方法，并进一步提高了其下游任务性能和与用户意图的对齐（Christiano 等，2017）。</p>

<p class="english">Generally, RLHF methods first fit a reward model to human preferences, then fine-tune a language model to maximize the inferred reward using RL algorithms.</p>
<p class="chinese">通常，RLHF方法首先将奖励模型拟合到人类偏好上，然后使用RL算法对语言模型进行微调，以最大化推测的奖励。</p>

<p class="english">Reward models tend to be an imperfect estimate of the true reward due to misspecification (Bıyık et al., 2022) and misgeneralization (Tien et al., 2023), and imperfect in reward models leads to reward hacking (Skalse et al., 2022).</p>
<p class="chinese">由于模型设定错误（Bıyık 等，2022）和过度泛化（Tien 等，2023），奖励模型往往是对真实奖励的一个不完美估计，而奖励模型的不完美会导致奖励劫持（Skalse 等，2022）。</p>

<p class="english">Methods with reward ensemble (Coste et al., 2024) and diverse feedback (Yu et al., 2023) are proposed to tackle this issue.</p>
<p class="chinese">为解决这一问题，提出了使用奖励集成（Coste 等，2024）和多样化反馈（Yu 等，2023）的方法。</p>

<p class="english">Our method retrains the reward model with policy samples to make it on-distribution and generalize to the policy’s data distribution.</p>
<p class="chinese">我们的方法通过使用策略样本重新训练奖励模型，使其保持在数据分布上，并能泛化到策略的数据分布。</p>

<p class="english">Human feedback simulation aims to generate additional synthetic preference data using weak human supervision and LLMs (Bai et al., 2022b).</p>
<p class="chinese">人类反馈模拟旨在利用弱人工监督和LLMs生成额外的合成偏好数据（Bai 等，2022b）。</p>

<p class="english">RLAIF approaches obtain pairwise preferences by scoring two outputs from a shared prompt (Lee et al., 2023), whereas RLCD generates outputs from two variants of a prompt (Yang et al., 2024).</p>
<p class="chinese">RLAIF方法通过评分来自共享提示的两个输出来获得成对的偏好（Lee 等，2023），而RLCD则从两个变体的提示中生成输出（Yang 等，2024）。</p>

<p class="english">Our method RLP-SPG is the first attempt to simulate human preferences using a set of outputs.</p>
<p class="chinese">我们的方法RLP-SPG是首次尝试使用一组输出模拟人类偏好。</p>

<p class="english">Uncertainty quantification provides confidence scores for generations of LLMs, helping users decide when to trust these generation results (Si et al., 2023).</p>
<p class="chinese">不确定性量化为LLMs的生成结果提供置信度评分，帮助用户决定何时信任这些生成结果（Si 等，2023）。</p>

<p class="english">Supervised methods fine-tune the language model to predict the uncertainty (Kadavath et al., 2022; Lin et al., 2022), while unsupervised methods measure uncertainty by calculating semantic entropy or semantic dispersion amongst generated answers (Kuhn et al., 2023; Lin et al., 2023).</p>
<p class="chinese">监督方法通过微调语言模型来预测不确定性（Kadavath 等，2022；Lin 等，2022），而无监督方法通过计算生成答案的语义熵或语义分散来衡量不确定性（Kuhn 等，2023；Lin 等，2023）。</p>

<p class="english">In this work, we measure uncertainty to selectively generate preference data.</p>
<p class="chinese">在本研究中，我们衡量不确定性以选择性地生成偏好数据。</p>
<p class="english">3 Preliminaries</p>
<p class="chinese">3 预备知识</p>
<p class="english">We start by introducing the instruction following task (Ouyang et al., 2022; Bai et al., 2022a). Given user instructions \( x \in X \) (e.g., “Generate a definition for artificial intelligence”), we aim to develop a model \( \pi_{\theta} \) that generates high-quality responses \( y \sim \pi_{\theta}(y|x) \) as judged by some latent reward model.</p>
<p class="chinese">我们首先介绍指令跟随任务（Ouyang 等，2022；Bai 等，2022a）。给定用户指令 \( x \in X \)（例如，“生成人工智能的定义”），我们的目标是开发一个模型 \( \pi_{\theta} \)，该模型根据某些潜在的奖励模型生成高质量的响应 \( y \sim \pi_{\theta}(y|x) \)。</p>


<p class="english">In this study, we focus on RLHF for this task, due to its central role in instruction-following LLMs (Ouyang et al., 2022).</p>
<p class="chinese">在本研究中，我们专注于该任务的RLHF，因为它在指令跟随LLMs中的核心作用（Ouyang 等，2022）。</p>

<p class="english">RLHF usually consists of three steps: human preference collecting, reward modeling, and RL policy optimization (Dubois et al., 2024; Rafailov et al., 2024; Casper et al., 2023).</p>
<p class="chinese">RLHF通常包括三个步骤：人类偏好收集、奖励建模和RL策略优化（Dubois 等，2024；Rafailov 等，2024；Casper 等，2023）。</p>

<p class="english">Step 0, SFT: RLHF generally begins with a pretrained model, which is fine-tuned with supervised learning on instruction-following demonstrations\( (x, y) \), to produce a model \( \pi^{\text{SFT}}(y|x) \).</p>
<p class="chinese">步骤0，SFT：RLHF通常从一个预训练模型开始，该模型通过监督学习对指令跟随示例 \( (x, y) \)进行微调，以生成模型 \( \pi^{\text{SFT}}(y|x) \)。</p>

<p class="english">Step 1, Human preference collecting: The first step is to produce pairs of responses \( (y_1, y_2) \sim \pi^{\text{SFT}}(y|x) \) for the instruction \( x \). These are then presented to humans who express preferences for each response, denoted as \( y_w \succ y_l | x \), where \( y_w \) and \( y_l \) denote the preferred and non-preferred completion amongst \( (y_1, y_2) \) respectively.</p>

<p class="chinese">步骤1，人类偏好收集：第一步是为指令 \( x \) 生成一对响应 \( (y_1, y_2) \sim \pi^{\text{SFT}}(y|x) \)。然后，这些响应会呈现给人类，要求他们对每个响应表示偏好，表示为 \( y_w \succ y_l | x \)，其中 \( y_w \) 和 \( y_l \) 分别表示在 \( (y_1, y_2) \) 中的首选和非首选完成。</p>

<p class="english">Step 2, Reward learning: The second step is to fit a reward model \( r_{\phi}(x, y) \) by minimizing the negative log-likelihood loss (Christiano et al., 2017):</p>
<p class="english">\[
L_R = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma(r_{\phi}(x, y_w) - r_{\phi}(x, y_l)) \right],
\]</p>
<p class="english">where \( D = \{(x, y_w, y_l)\} \) is a dataset of pairwise preferences and \( \sigma \) is the sigmoid function. \( r_{\phi}(x, y) \) is often initialized from \( \pi^{\text{SFT}}(y|x) \) with one additional linear layer that infers the reward value.</p>

<p class="chinese">步骤2，奖励学习：第二步是通过最小化负对数似然损失来拟合奖励模型 \( r_{\phi}(x, y) \)（Christiano 等，2017）：</p>
<p class="chinese">\[
L_R = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma(r_{\phi}(x, y_w) - r_{\phi}(x, y_l)) \right],
\]</p>
<p class="chinese">其中 \( D = \{(x, y_w, y_l)\} \) 是成对偏好的数据集，\( \sigma \) 是 sigmoid 函数。 \( r_{\phi}(x, y) \) 通常从 \( \pi^{\text{SFT}}(y|x) \) 初始化，并增加一个线性层来推断奖励值。</p>


<p class="english">Step 3, RL policy optimization: The third step is to use the reward model \( r_{\phi}(x, y) \) to fine-tune the language model. The parameters \( \theta \) of \( \pi \) are trained to maximize</p>
<p class="english">\[
\mathbb{E}_{x \sim U, y \sim \pi_{\theta}(y|x)} \left[ r_{\phi}(x, y) - \beta D_{\text{KL}}(\pi_{\theta}(y|x) || \pi_{\text{ref}}(y|x)) \right],
\]</p>
<p class="english">where \( U = \{x\} \) is an unlabeled instruction dataset, the language model policy \( \pi_{\theta}(y|x) \) is fine-tuned from the SFT model \( \pi^{\text{SFT}} \), the reference policy \( \pi_{\text{ref}} \) is also the SFT model \( \pi^{\text{SFT}} \), and \( \beta \) is a regularization coefficient controlling the deviation from \( \pi_{\text{ref}} \). This objective is typically optimized with RL algorithms such as PPO (Schulman et al., 2017).</p>

<p class="chinese">步骤3，RL策略优化：第三步是使用奖励模型 \( r_{\phi}(x, y) \) 来微调语言模型。模型 \( \pi \) 的参数 \( \theta \) 通过训练最大化：</p>
<p class="chinese">\[
\mathbb{E}_{x \sim U, y \sim \pi_{\theta}(y|x)} \left[ r_{\phi}(x, y) - \beta D_{\text{KL}}(\pi_{\theta}(y|x) || \pi_{\text{ref}}(y|x)) \right],
\]</p>
<p class="chinese">其中 \( U = \{x\} \) 是一个无标签的指令数据集，语言模型策略 \( \pi_{\theta}(y|x) \) 是从 SFT 模型 \( \pi^{\text{SFT}} \) 微调得到的，参考策略 \( \pi_{\text{ref}} \) 也是 SFT 模型 \( \pi^{\text{SFT}} \)，而 \( \beta \) 是控制偏离 \( \pi_{\text{ref}} \) 的正则化系数。这个目标通常通过 RL 算法如 PPO（Schulman 等，2017）来优化。</p>

<p class="english">4 Reward Learning on Policy</p>
<p class="chinese">4 基于策略的奖励学习</p>

<p class="english">4.1 Overview</p>
<p class="chinese">4.1 概述</p>

<p class="english">In this study, we propose a novel RLHF framework to fine-tune LLMs with human feedback following five steps: Step 1-3. Collect a pairwise human preference dataset D, then train a reward model rϕ and fine-tune a language model policy πθ; Step 4. Retrain a reward model rˆϕ using outputs of policy πθ; Step 5. Retrain a policy πˆθ based on the retrained reward model rˆϕ.</p>
<p class="chinese">在本研究中，我们提出了一种新的 RLHF 框架，通过五个步骤使用人类反馈来微调 LLMs：步骤 1-3：收集一对一的人类偏好数据集 D，然后训练一个奖励模型 rϕ 并微调语言模型策略 πθ；步骤 4：使用策略 πθ 的输出重新训练奖励模型 rˆϕ；步骤 5：基于重新训练的奖励模型 rˆϕ 重新训练策略 πˆθ。</p>

<p class="english">Before applying RLP, we assume existing RLHF approaches can be used to train the reward model rϕ and the policy πθ (Ouyang et al., 2022). The sample distribution of the policy πθ can be quite different from the preference data D on which the reward model rϕ is trained (Touvron et al., 2023b).</p>
<p class="chinese">在应用 RLP 之前，我们假设可以使用现有的 RLHF 方法来训练奖励模型 rϕ 和策略 πθ（Ouyang 等人，2022）。策略 πθ 的样本分布可能与训练奖励模型 rϕ 的偏好数据 D 存在较大差异（Touvron 等人，2023b）。</p>

<p class="english">For example, outputs become increasingly longer after applying RLHF methods as shown in the analysis of AlpacaFarm (Dubois et al., 2024). The average length of SFT outputs is 278 characters and applying PPO increases it to 637 tokens. These distributional differences make the reward model rϕ inaccurate off-distribution.</p>
<p class="chinese">例如，在 AlpacaFarm 的分析中显示，应用 RLHF 方法后，输出会变得越来越长（Dubois 等人，2024）。SFT 输出的平均长度为 278 个字符，而应用 PPO 后其长度增加到 637 个标记。这些分布差异导致奖励模型 rϕ 在偏离分布时不准确。</p>

<p class="english">Our goal is to refine the reward model using samples of the policy πθ and keep it on-distribution. This process is expected to increase the generalization of the retrained reward model rˆϕ to policy samples. Accordingly, it can maintain an accurate reward during the RL policy optimization phase.</p>
<p class="chinese">我们的目标是使用策略 πθ 的样本来优化奖励模型，并使其保持在分布上。这个过程预计能够提高重新训练的奖励模型 rˆϕ 对策略样本的泛化能力。因此，它可以在 RL 策略优化阶段保持准确的奖励。</p>
<h2>4 Reward Learning on Policy</h2>
    <p class="chinese">4 基于策略的奖励学习</p>
    <h3>4.1 Overview</h3>
    <p class="chinese">4.1 概述</p>

    <p class="english">
        In this study, we propose a novel RLHF framework to fine-tune LLMs with human feedback following five steps: Step 1-3. Collect a pairwise human preference dataset \( D \), then train a reward model \( r_\phi \) and fine-tune a language model policy \( \pi_\theta \); Step 4. Retrain a reward model \( \hat{r}_\phi \) using outputs of policy \( \pi_\theta \); Step 5. Retrain a policy \( \hat{\pi}_\theta \) based on the retrained reward model \( \hat{r}_\phi \).
    </p>
    <p class="chinese">
        在本研究中，我们提出了一种新的 RLHF 框架，通过五个步骤使用人类反馈来微调 LLMs：步骤 1-3：收集一对一的人类偏好数据集 \( D \)，然后训练一个奖励模型 \( r_\phi \) 并微调语言模型策略 \( \pi_\theta \)；步骤 4：使用策略 \( \pi_\theta \) 的输出重新训练奖励模型 \( \hat{r}_\phi \)；步骤 5：基于重新训练的奖励模型 \( \hat{r}_\phi \) 重新训练策略 \( \hat{\pi}_\theta \)。
    </p>

    <p class="english">
        Before applying RLP, we assume existing RLHF approaches can be used to train the reward model \( r_\phi \) and the policy \( \pi_\theta \) (Ouyang et al., 2022). The sample distribution of the policy \( \pi_\theta \) can be quite different from the preference data \( D \) on which the reward model \( r_\phi \) is trained (Touvron et al., 2023b).
    </p>
    <p class="chinese">
        在应用 RLP 之前，我们假设可以使用现有的 RLHF 方法来训练奖励模型 \( r_\phi \) 和策略 \( \pi_\theta \)（Ouyang 等人，2022）。策略 \( \pi_\theta \) 的样本分布可能与训练奖励模型 \( r_\phi \) 的偏好数据 \( D \) 存在较大差异（Touvron 等人，2023b）。
    </p>

    <p class="english">
        For example, outputs become increasingly longer after applying RLHF methods as shown in the analysis of AlpacaFarm (Dubois et al., 2024). The average length of SFT outputs is 278 characters and applying PPO increases it to 637 tokens. These distributional differences make the reward model \( r_\phi \) inaccurate off-distribution.
    </p>
    <p class="chinese">
        例如，在 AlpacaFarm 的分析中显示，应用 RLHF 方法后，输出会变得越来越长（Dubois 等人，2024）。SFT 输出的平均长度为 278 个字符，而应用 PPO 后其长度增加到 637 个标记。这些分布差异导致奖励模型 \( r_\phi \) 在偏离分布时不准确。
    </p>

    <p class="english">
        Our goal is to refine the reward model using samples of the policy \( \pi_\theta \) and keep it on-distribution. This process is expected to increase the generalization of the retrained reward model \( \hat{r}_\phi \) to policy samples. Accordingly, it can maintain an accurate reward during the RL policy optimization phase.
    </p>
    <p class="chinese">
        我们的目标是使用策略 \( \pi_\theta \) 的样本来优化奖励模型，并使其保持在分布上。这个过程预计能够提高重新训练的奖励模型 \( \hat{r}_\phi \) 对策略样本的泛化能力。因此，它可以在 RL 策略优化阶段保持准确的奖励。
    </p>
<h2>4.2 Reward Retraining</h2>
    <p class="english">4.2 Reward Retraining</p>
    <p class="chinese">4.2 奖励重新训练</p>

    <p class="english">
        We now describe how to retrain the reward model \( \hat{r}_\phi \) in Step 4 of RLP. We first construct a dataset of policy samples \( P = \{(x, y) \mid x \in U, y \sim \pi_\theta(y|x)\} \), where \( y \) is a set of \( n \) outputs from policy \( \pi_\theta \) for instruction \( x \). Then, we refine the reward model with policy samples \( P \) in addition to the human preference dataset \( D \). Specifically, we propose two different methods for this purpose: unsupervised multi-view learning (UML) and synthetic preference generation (SPG).
    </p>
    <p class="chinese">
        现在，我们描述如何在 RLP 的步骤 4 中重新训练奖励模型 \( \hat{r}_\phi \)。我们首先构建一个策略样本数据集 \( P = \{(x, y) \mid x \in U, y \sim \pi_\theta(y|x)\} \)，其中 \( y \) 是来自策略 \( \pi_\theta \) 对指令 \( x \) 的 \( n \) 个输出的集合。然后，我们在现有的人类偏好数据集 \( D \) 的基础上，使用策略样本 \( P \) 来优化奖励模型。具体来说，我们提出了两种方法来实现这一目标：无监督多视图学习（UML）和合成偏好生成（SPG）。
    </p>

    <p class="english">
        Unsupervised Multi-View Learning attempts to learn robust representations of policy samples. For each pair \( (x, y) \in P \), two semantic invariant views are constructed: \( v_i(x) = (x, y) \mid y \sim y \), (i = 1, 2). These two views preserve the same task-relevant information (Zhao et al., 2017). Then, a multi-view information bottleneck (MIB) loss (Federici et al., 2020) is optimized for unsupervised representation learning, following the information bottleneck principle (Tishby et al., 2000). This optimization process retains task-relevant information in the representations while discarding superficial information.
    </p>
    <p class="chinese">
        无监督多视图学习旨在学习策略样本的鲁棒表示。对于每个 \( (x, y) \in P \) 对，构建了两个语义不变视图：\( v_i(x) = (x, y) \mid y \sim y \)，(i = 1, 2)。这两个视图保留了相同的任务相关信息（Zhao 等人，2017）。然后，优化多视图信息瓶颈（MIB）损失（Federici 等人，2020）以进行无监督表示学习，遵循信息瓶颈原理（Tishby 等人，2000）。该优化过程在表示中保留任务相关信息，同时丢弃表面信息。
    </p>

    <p class="english">
        To facilitate the computation, we parametrize the representation \( z_i \) of each view \( v_i(x) \) with a factorized Gaussian distribution, i.e., \( p_\psi(z|v_i) = \mathcal{N} [\mu(v_i), \Sigma(v_i)] \). Concretely, we estimate \( v_i(x) \) with the final transformer layer of the reward model and use two neural networks \( \mu(v_i) \) and \( \Sigma(v_i) \) to produce the mean and deviation respectively. The following MIB loss is optimized:
    </p>
    <p class="chinese">
        为了简化计算，我们将每个视图 \( v_i(x) \) 的表示 \( z_i \) 参数化为一个因子化的高斯分布，即 \( p_\psi(z|v_i) = \mathcal{N} [\mu(v_i), \Sigma(v_i)] \)。具体地，我们通过奖励模型的最终变换层估计 \( v_i(x) \)，并使用两个神经网络 \( \mu(v_i) \) 和 \( \Sigma(v_i) \) 分别生成均值和方差。然后，优化以下的 MIB 损失：
    </p>

    <p class="english">
        \[
        L_M = \mathbb{E}_{(x,y) \sim P} \left[ -I(z_1; z_2) + D_{\text{SKL}}(p_\psi(z|v_1) \parallel p_\psi(z|v_2)) \right]
        \]
        where \( I \) calculates mutual information of two random variables, and \( D_{\text{SKL}} \) represents the symmetrized KL divergence obtained by averaging the expected value of \( D_{\text{KL}}(p_\psi(z|v_1) \parallel p_\psi(z|v_2)) \) and \( D_{\text{KL}}(p_\psi(z|v_2) \parallel p_\psi(z|v_1)) \).
    </p>
    <p class="chinese">
        \[
        L_M = \mathbb{E}_{(x,y) \sim P} \left[ -I(z_1; z_2) + D_{\text{SKL}}(p_\psi(z|v_1) \parallel p_\psi(z|v_2)) \right]
        \]
        其中，\( I \) 计算两个随机变量的互信息，\( D_{\text{SKL}} \) 表示对称 KL 散度，通过对 \( D_{\text{KL}}(p_\psi(z|v_1) \parallel p_\psi(z|v_2)) \) 和 \( D_{\text{KL}}(p_\psi(z|v_2) \parallel p_\psi(z|v_1)) \) 的期望值进行平均来得到。
    </p>
    <p class="english">
        Synthetic Preference Generation aims to simulate high-quality preference data with policy samples. For each pair \( (x, y) \in P \), we assume the most frequent item of \( y \) as the correct prediction and its frequency as the confidence score (Si et al., 2023), following the self-consistency assumption (Wang et al., 2023). To address semantic equivalence, i.e., different sentences can mean the same thing, we cluster items of \( y \) into groups \( G \) with a bi-directional entailment algorithm (Kuhn et al., 2023). Sentences from each group \( g \in G \) are expected to share the same meaning. We estimate the confidence score of \( (x, y) \) as:
    </p>
    <p class="english">
        \[
        \frac{| \tilde{g} |}{| y |}
        \]
    </p>
    <p class="chinese">
        合成偏好生成旨在通过策略样本模拟高质量的偏好数据。对于每一对 \( (x, y) \in P \)，我们假设 \( y \) 中最频繁的项为正确预测，并将其频率作为置信度（Si 等人，2023），遵循自一致性假设（Wang 等人，2023）。为了解决语义等价问题，即不同的句子可以表达相同的含义，我们使用双向蕴含算法（Kuhn 等人，2023）将 \( y \) 中的项聚类成组 \( G \)。每个组 \( g \in G \) 中的句子预计具有相同的含义。我们估计 \( (x, y) \) 的置信度为：
    </p>
    
    <p class="chinese">
        \[
        \frac{| \tilde{g} |}{| y |}
        \]
    </p>
    
    <p class="english">
        where \( \tilde{g} \) is the largest group of \( G \) and the operator \( | \cdot | \) measures the size of a set. Thus, we can selectively generate a synthetic preference dataset with high confidences \( \hat{D} = \{(x, y_w, y_l) \mid (x, y) \in P, \frac{| \tilde{g} |}{| y |} \geq \gamma, y_w \sim \tilde{g}, y_l \sim y \setminus \tilde{g} \} \), where \( \gamma \) is the threshold for selective generation, the preferred output \( y_w \) is sampled from the largest group \( \tilde{g} \) with the largest reward score, and the non-preferred one \( y_l \) is randomly sampled from the rest of the groups.
    </p>
    <p class="chinese">
        其中，\( \tilde{g} \) 是 \( G \) 中最大的组，运算符 \( | \cdot | \) 表示集合的大小。因此，我们可以选择性地生成具有高置信度的合成偏好数据集 \( \hat{D} = \{(x, y_w, y_l) \mid (x, y) \in P, \frac{| \tilde{g} |}{| y |} \geq \gamma, y_w \sim \tilde{g}, y_l \sim y \setminus \tilde{g} \} \)，其中 \( \gamma \) 是选择性生成的阈值，优选输出 \( y_w \) 从具有最大奖励分数的最大组 \( \tilde{g} \) 中抽样，非优选输出 \( y_l \) 随机从其余组中抽样。
    </p>
    
    <p class="english">
        The overall loss that we optimize for the reward model \( \hat{r}_\phi \) is:
    </p>
    
    <p class="english">
        \[
        L_{\hat{R}} = - \mathbb{E}_{(x, y_w, y_l) \sim D \cup \hat{D}} \left[ \log \sigma(\hat{r}_\phi(x, y_w) - \hat{r}_\phi(x, y_l)) \right] + \lambda L_M \quad (1)
        \]
    </p>
    <p class="chinese">
        我们为奖励模型 \( \hat{r}_\phi \) 优化的总体损失为：
    </p>
    
    <p class="chinese">
        \[
        L_{\hat{R}} = - \mathbb{E}_{(x, y_w, y_l) \sim D \cup \hat{D}} \left[ \log \sigma(\hat{r}_\phi(x, y_w) - \hat{r}_\phi(x, y_l)) \right] + \lambda L_M \quad (1)
        \]
    </p>
    
    
    <p class="english">
        where the coefficient \( \lambda \) controls the weight of the multi-view information bottleneck loss. To simplify computational complexity, we implement two variants: 
        1. RLP-UML removes the synthetic dataset \( \hat{D} \) in Eq. 1 and learns the representations of policy samples when fitting the reward model. 
        2. RLP-SPG removes the MIB loss by setting \( \lambda = 0 \) in Eq. 1 and fits the reward model with human and synthetic preference data.
    </p>
    <p class="chinese">
        其中，系数 \( \lambda \) 控制多视图信息瓶颈损失的权重。为了简化计算复杂度，我们实现了两种变体：
        1. RLP-UML 移除了公式 (1) 中的合成数据集 \( \hat{D} \)，并在拟合奖励模型时学习策略样本的表示。
        2. RLP-SPG 通过在公式 (1) 中设置 \( \lambda = 0 \) 来移除 MIB 损失，并使用人类和合成偏好数据拟合奖励模型。
    </p>
    
    <p class="english">4.3 Policy Retraining</p>
    <p class="chinese">4.3 策略重新训练</p>
    
    <p class="english">
        We finally retrain the policy \( \hat{\pi}_{\theta} \) using \( \hat{r}_\phi \) in Step 5 of RLP. Specifically, we optimize \( \hat{\pi}_{\theta} \) to maximize
        <br>
        \( \mathbb{E}_{x \sim U, y \sim \hat{\pi_{\theta}}(y|x)} \left[ \hat{r}_\phi(x, y) - \beta D_{\text{KL}} \left( \hat{\pi}_{\theta}(y|x) \parallel \pi_{\text{ref}}(y|x) \right) \right] \).
    </p>
    <p class="chinese">
        最后，我们在 RLP 的第 5 步中使用 \( \hat{r}_\phi \) 重新训练策略 \( \hat{\pi}_{\theta}\)。具体地，我们优化 \( \hat{\pi}_{\theta} \) 以最大化
        <br>
        \( \mathbb{E}_{x \sim U, y \sim \hat{\pi_{\theta}}(y|x)} \left[ \hat{r}_\phi(x, y) - \beta D_{\text{KL}} \left( \hat{\pi}_{\theta}(y|x) \parallel \pi_{\text{ref}}(y|x) \right) \right] \)。
    </p>
    
    <p class="english">Our approach RLP is summarized in Algorithm 1.</p>
    <p class="chinese">我们的 RLP 方法总结在算法 1 中。</p>
    
    <p class="english"><b>Algorithm 1: RLP: RLHF with Reward Learning on Policy</b></p>
<p class="chinese"><b>算法 1：RLP：基于策略的奖励学习的 RLHF</b></p>

<p class="english"><b>Input:</b> SFT model \( \pi^{\text{SFT}} \), unlabeled data \( U \). </p>
<p class="chinese"><b>输入：</b> SFT 模型 \( \pi^{\text{SFT}} \)，未标注数据集 \( U \)。</p>

<p class="english"><b>Output:</b> A language model policy \( \hat{\pi}_{\theta} \).</p>
<p class="chinese"><b>输出：</b> 语言模型策略 \( \hat{\pi}_{\theta} \)。</p>

<table class="algorithm">
  <tr>
    <th>Step</th>
    <th>Action</th>
  </tr>
  <tr>
    <td>1</td>
    <td>Collect a human preference dataset \( D \).<br>收集人类偏好数据集 \( D \)。</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Train a reward model \( r_{\phi} \) using \( D \).<br>使用数据集 \( D \) 训练奖励模型 \( r_{\phi} \)。</td>
  </tr>
  <tr>
    <td>3</td>
    <td>Fine-tune a language model \( \pi_{\theta} \) from \( \pi^{\text{SFT}} \) using \( U \) and \( r_{\phi} \).<br>使用数据集 \( U \) 和奖励模型 \( r_{\phi} \) 从 \( \pi^{\text{SFT}} \) 微调语言模型 \( \pi_{\theta} \)。</td>
  </tr>
  <tr>
    <td>4</td>
    <td>Retrain a reward model \( \hat{r}_{\phi} \) using \( L_{\hat{R}} \) (Eq. 1).<br>使用 \( L_{\hat{R}} \)（公式 1）重新训练奖励模型 \( \hat{r}_{\phi} \)。</td>
  </tr>
  <tr>
    <td>5</td>
    <td>Fine-tune \( \hat{\pi}_{\theta} \) from \( \pi^{\text{SFT}} \) using \( U \) and \( \hat{r}_{\phi} \).<br>使用数据集 \( U \) 和重新训练的奖励模型 \( \hat{r}_{\phi} \) 从 \( \pi^{\text{SFT}} \) 微调 \( \hat{\pi}_{\theta} \)。</td>
  </tr>
</table>

<p class="english">Table 1: Dataset statistics.</p>
<p class="chinese">表格 1：数据集统计信息</p>

<p>
  <img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/4-Table1-1.png" alt="Figure 2" width="600">
</p>
<table class="dataset">
  <tr>
    <th>Dataset</th>
    <th>#Samples</th>
  </tr>
  <tr>
    <td>Training data</td>
    <td>训练数据</td>
  </tr>
  <tr>
    <td>SFT dataset</td>
    <td>10k</td>
  </tr>
  <tr>
    <td>Preference dataset \( D \)</td>
    <td>10k</td>
  </tr>
  <tr>
    <td>Unlabeled dataset \( U \)</td>
    <td>20k</td>
  </tr>
  <tr>
    <td>Evaluation data</td>
    <td>评估数据</td>
  </tr>
  <tr>
    <td>AlpacaFarm</td>
    <td>805</td>
  </tr>
  <tr>
    <td>LLMBar</td>
    <td>100</td>
  </tr>
  <tr>
    <td>Vicuna</td>
    <td>80</td>
  </tr>
</table>
<p class="english"><b>5 Experiments</b></p>
<p class="chinese"><b>5 实验</b></p>

<p class="english"><b>5.1 Datasets</b></p>
<p class="chinese"><b>5.1 数据集</b></p>

<p class="english">
We run experiments on the instruction following task (Ouyang et al., 2022; Bai et al., 2022a), which remains a challenging task for the strongest LLMs today (Wu et al., 2023; Li et al., 2023). Training data of the RLHF procedure come from Alpaca data, which consists of 52k instruction-following demonstrations \( (x, y) \) (Taori et al., 2023). Following the data splits of AlpacaFarm (Dubois et al., 2024), three splits are used: 1. SFT split are 10k data for fine-tuning the SFT model \( \pi^{\text{SFT}} \); 2. Preference split are 10k instructions on which we collect pairwise preference dataset \( D \); 3. Unlabeled split are 20k unlabeled instructions \( U \) used in PPO. Concretely, two variants of preference dataset \( D \) are curated: simulated \( D_{\text{sim}} \) are constructed with AlpacaFarm simulated annotators by prompting API LLMs, and human \( D_{\text{human}} \) are constructed with human annotators. Evaluation data of the trained LLMs include three benchmarks: 1. AlpacaFarm consists of 805 instructions, which includes instructions from Self-instruct evaluation (Wang et al., 2022), OASST evaluation, Anthropic helpful evaluation (Bai et al., 2022a), Vicuna evaluation (Chiang and Lee, 2023), and Koala evaluation (Geng et al., 2023); 2. LLMBar (Zeng et al., 2024) contains 100 instructions with objective preferences. We use the natural set to evaluate performance in real-world distributions; 3. Vicuna (Chiang et al., 2023) contains 80 instructions to evaluate different skills of LLMs. Table 1 shows the statistics of these datasets.
</p>

<p class="chinese">
我们在指令跟随任务（Ouyang 等，2022；Bai 等，2022a）上进行实验，该任务对于当前最强的 LLMs 仍然是一个具有挑战性的任务（Wu 等，2023；Li 等，2023）。RLHF 过程的训练数据来自 Alpaca 数据，包含 52k 个指令跟随示例 \( (x, y) \)（Taori 等，2023）。按照 AlpacaFarm（Dubois 等，2024）中的数据拆分，使用了三个拆分：1. SFT 拆分包含 10k 数据用于微调 SFT 模型 \( \pi^{\text{SFT}} \)；2. 偏好拆分包含 10k 条指令，我们收集一对一偏好数据集 \( D \)；3. 无标签拆分包含 20k 条无标签指令 \( U \)，用于 PPO。具体而言，偏好数据集 \( D \) 有两种变体：模拟的 \( D_{\text{sim}} \) 由 AlpacaFarm 模拟注释器通过调用 API LLMs 生成，人工的 \( D_{\text{human}} \) 则由人工注释者构建。训练过的 LLMs 的评估数据包含三个基准：1. AlpacaFarm 包含 805 条指令，包括来自 Self-instruct 评估（Wang 等，2022）、OASST 评估、Anthropic helpful 评估（Bai 等，2022a）、Vicuna 评估（Chiang 和 Lee，2023）和 Koala 评估（Geng 等，2023）的指令；2. LLMBar（Zeng 等，2024）包含 100 条具有客观偏好的指令。我们使用自然集来评估在真实世界分布中的表现；3. Vicuna（Chiang 等，2023）包含 80 条指令，用于评估 LLMs 的不同技能。表格 1 显示了这些数据集的统计信息。
</p>

<p class="english"><b>5.2 Metrics and Experimental Setups</b></p>
<p class="chinese"><b>5.2 评估指标与实验设置</b></p>

<p class="english">
Following Dubois et al. (2024); Touvron et al. (2023b), we use win-rate to evaluate the performance of an LLM \( \pi_{\theta} \), i.e., the percentage of times \( \pi_{\theta} \) is preferred to a reference model \( \pi_{\text{ref}} \) for their instruction-following outputs. Following the experimental setups of AlpacaFarm (Dubois et al., 2024), we use simulated win-rate to evaluate methods trained on simulated \( D_{\text{sim}} \) with simulated annotators by prompting API LLMs. In parallel, we use human win-rate to evaluate methods trained on human \( D_{\text{human}} \) with human annotators.
</p>

<p class="chinese">
按照 Dubois 等（2024）；Touvron 等（2023b）的方法，我们使用胜率来评估 LLM \( \pi_{\theta} \) 的表现，即 \( \pi_{\theta} \) 在指令跟随输出中相对于参考模型 \( \pi_{\text{ref}} \) 被偏好的百分比。根据 AlpacaFarm（Dubois 等，2024）的实验设置，我们使用模拟胜率来评估在模拟的 \( D_{\text{sim}} \) 上训练的模型，该数据集是通过调用 API LLMs 并由模拟注释者生成的。同时，我们使用人工胜率来评估在人工注释数据集 \( D_{\text{human}} \) 上训练的模型，该数据集由人工注释者构建。
</p>

    

<p class="english">Table 2: The win-rate (%) performance of RLP and baselines. Win-rates are computed against reference model text-davinci-003. Baseline results in AlpacaFarm come from Dubois et al. (2024). Bold numbers are superior results among the implemented LLMs. We omitted LLMBar and Vicuna for human evaluation because the simulated method rankings consistently correlate with the human method rankings in AlpacaFarm.</p>
<p class="chinese">表 2：RLP 和基准模型的胜率（%）表现。胜率是相对于参考模型 text-davinci-003 计算的。AlpacaFarm 中的基准结果来自 Dubois 等人（2024）。加粗的数字表示在实现的 LLMs 中优于其他模型的结果。我们省略了 LLMBar 和 Vicuna 的人工评估，因为在 AlpacaFarm 中，模拟方法的排名与人工方法的排名一致。</p>

<p>
  <img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/5-Table2-1.png" alt="Figure 2" width="600">
</p>
<p>Simulated Win-Rate模拟胜率Human Win-Rate人类胜率</p>

<p class="english">Figure 2: The simulated win-rate (%) performance of RLP-SPG compared to PPO on various subsets of AlpacaFarm. Win-rates are computed against reference model text-davinci-003.</p>
<p class="chinese">图 2：RLP-SPG 相较于 PPO 在 AlpacaFarm 各个子集上的模拟胜率（%）表现。胜率是相对于参考模型 text-davinci-003 计算的。</p>

<p>Tie平局</p>

<p>
  <img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/5-Figure2-1.png" alt="Figure 3" width="600">
</p>



<p class="english">5.3 Implementation Details</p>
<p class="chinese">5.3 实现细节</p>

<p class="english">Our implementations of reward learning, policy optimization, and simulated annotators are based on the AlpacaFarm codebase and its default hyperparameters (Dubois et al., 2024). Specifically, we use RL algorithm PPO (Schulman et al., 2017) to fine-tune the language model during policy optimization. All reward models and language models are based on LLaMA-7B (Touvron et al., 2023a). We sample n = 10 outputs for each instruction to construct a dataset of policy samples <i>P</i>. For unsupervised multi-view learning, we implement <i>µ(v<sub>i</sub>)</i> and <i>Σ(v<sub>i</sub>)</i> as three-layer MLPs, and use Jensen-Shannon mutual information estimator (Hjelm et al., 2018) to estimate mutual information <i>I</i> in the MIB loss. For synthetic preference generation, we implement a bidirectional entailment clustering algorithm using Deberta-large model (He et al., 2021) and set the threshold <i>γ = 0.5</i> for selective generation. We set <i>λ = 0.5</i> in Eq. (1) for RLP-UML. At training and inference time, we use a sampling temperature of 1.0 and 0.7, respectively. All experiments are performed on a single 8×A100 machine.</p>

<p class="chinese">我们的奖励学习、策略优化和模拟注释员实现基于 AlpacaFarm 代码库及其默认超参数（Dubois 等人，2024）。具体来说，我们使用强化学习算法 PPO（Schulman 等人，2017）来在策略优化过程中微调语言模型。所有奖励模型和语言模型均基于 LLaMA-7B（Touvron 等人，2023a）。我们为每个指令采样 <i>n = 10</i> 个输出，构建策略样本数据集 <i>P</i>。对于无监督多视图学习，我们将 <i>µ(v<sub>i</sub>)</i> 和 <i>Σ(v<sub>i</sub>)</i> 实现为三层 MLP，并使用 Jensen-Shannon 互信息估计器（Hjelm 等人，2018）来估算 MIB 损失中的互信息 <i>I</i>。对于合成偏好生成，我们使用 Deberta-large 模型（He 等人，2021）实现双向蕴含聚类算法，并设置选择性生成阈值 <i>γ = 0.5</i>。在 Eq. (1) 中，我们为 RLP-UML 设置了 <i>λ = 0.5</i>。在训练和推理时，我们使用的采样温度分别为 1.0 和 0.7。所有实验均在单台 8×A100 机器上执行。</p>




<p class="english">5.4 Baselines</p>
<p class="chinese">5.4 基准对比</p>

<p class="english">We compare RLP with competitive baselines: 1. LLaMA-7B (Touvron et al., 2023a) directly generates outputs using the base unaligned LLaMA-7B; 2. SFT (Taori et al., 2023) is a LLaMA-7B model supervised fine-tuned on 10k Alpaca instruction-following data; 3. Best-of-n (Stiennon et al., 2020) samples n i.i.d. responses from the SFT model and returns the response with the highest inferred reward; 4. PPO (Schulman et al., 2017) is a reinforcement learning algorithm that maximizes surrogate reward, subject to a KL penalty keeping parameters near the SFT model; 5. ChatGPT uses OpenAI API LLM gpt-3.5-turbo-0301; 6. GPT-4 uses OpenAI API LLM gpt-4-0314.</p>

<p class="chinese">我们将 RLP 与以下竞争基准方法进行比较：1. LLaMA-7B（Touvron 等人，2023a）直接使用基础未对齐的 LLaMA-7B 生成输出；2. SFT（Taori 等人，2023）是一个在 10k Alpaca 指令数据上进行监督微调的 LLaMA-7B 模型；3. Best-of-n（Stiennon 等人，2020）从 SFT 模型中抽取 n 个独立同分布的响应，并返回推测奖励最高的响应；4. PPO（Schulman 等人，2017）是一种强化学习算法，通过最大化替代奖励，结合 KL 惩罚将参数保持在 SFT 模型附近；5. ChatGPT 使用 OpenAI API LLM gpt-3.5-turbo-0301；6. GPT-4 使用 OpenAI API LLM gpt-4-0314。</p>

<p class="english">5.5 Main Results</p>
<p class="chinese">5.5 主要结果</p>

<p class="english">We compare the win-rate performance of our method RLP and all baselines on three standard benchmarks to assess their instruction-following ability in Table 2. It can be seen that API LLM GPT-4 significantly outperforms all other models due to its obvious advantages. Among the implemented LLMs, RLP-SPG performs the best in both the simulator and human preference data, and achieves SOTA results on all three benchmarks. Compared with the implemented best-performing baseline PPO, RLP-SPG brings up from a simulator win-rate of 46.8% to 50.2% in AlpacaFarm, 47.5% to 50.5% in LLMBar, and 57.5% to 62.5% in Vicuna. RLP-SPG also brings up from a human win-rate of 55.1% to 57.4% in AlpacaFarm. We can also observe that: 1. Both the two variants of RLP, namely, RLP-UML and RLP-SPG, outperform all implemented baselines that do not train reward models using policy samples. The performance gain demonstrates the advantage of considering policy for reward learning, which can help keep the reward model on-distribution. 2. RLP-SPG generally outperforms RLP-UML under all circumstances. It demonstrates that synthetic preference generation leads to better performance, which simulates pairwise preference data with policy samples that can be leveraged for optimizing a reward model directly. To provide a clearer perspective on RLP’s superiority over other baselines, we illustrate the simulated win-rate of our best method RLP-SPG compared to the best-performing baseline PPO on various subsets of AlpacaFarm in Figure 2. Instructions from these subsets show diverse coverage over realistic interactions, allowing for an intricate analysis of the proficiency attained through language model fine-tuning (Dubois et al., 2024). Notably, RLP-SPG outperforms PPO across all subsets, including Self-instruct evaluation, Anthropic helpful evaluation, OASST evaluation, and Koala evaluation. It further indicates that reward learning on policy leads to a comprehensive enhancement in the capabilities of the LLMs. Meanwhile, RLP also outperforms these baselines on knowledge intensive benchmarks such as MMLU (Hendrycks et al., 2021) (See Appendix A.).</p>

<p class="chinese">我们比较了 RLP 方法与所有基准方法在三个标准基准上的胜率表现，以评估它们在指令跟随任务上的表现，结果见表 2。可以看出，API LLM GPT-4 由于其显著的优势，明显优于所有其他模型。在已实现的 LLM 中，RLP-SPG 在模拟数据和人类偏好数据中均表现最佳，并在所有三个基准上达到了 SOTA（最新技术水平）结果。与实现的最佳基准方法 PPO 相比，RLP-SPG 在 AlpacaFarm 中的模拟胜率从 46.8% 提升至 50.2%，在 LLMBar 中从 47.5% 提升至 50.5%，在 Vicuna 中从 57.5% 提升至 62.5%。RLP-SPG 还使得 AlpacaFarm 的人类胜率从 55.1% 提升至 57.4%。我们还可以观察到：1. RLP 的两个变体，即 RLP-UML 和 RLP-SPG，均优于所有未使用策略样本训练奖励模型的基准方法。性能提升证明了考虑策略进行奖励学习的优势，这有助于保持奖励模型在分布上的一致性。2. 在所有情况下，RLP-SPG 通常优于 RLP-UML。这表明合成偏好生成能带来更好的表现，它通过模拟带有策略样本的成对偏好数据来直接优化奖励模型。为了更清晰地展示 RLP 相比其他基准的优势，我们在图 2 中展示了我们最佳方法 RLP-SPG 与表现最佳的基准 PPO 在 AlpacaFarm 不同子集上的模拟胜率。来自这些子集的指令展示了现实互动的多样化覆盖，能够深入分析通过语言模型微调所获得的能力（Dubois 等人，2024）。值得注意的是，RLP-SPG 在所有子集上均优于 PPO，包括 Self-instruct 评估、Anthropic helpful 评估、OASST 评估和 Koala 评估。这进一步表明，基于策略的奖励学习能全面增强 LLM 的能力。同时，RLP 也在诸如 MMLU（Hendrycks 等人，2021）等知识密集型基准上优于这些基准方法（见附录 A）。</p>

<p class="english">Table 3: Outputs generated by RLP and baselines for an example from AlpacaFarm. RLP-UML and RLP-SPG produce more comprehensive outputs than SFT and PPO in this case.</p>
<p class="chinese">表格 3: RLP 和基准方法在 AlpacaFarm 示例中的生成输出。RLP-UML 和 RLP-SPG 在此案例中生成的输出比 SFT 和 PPO 更全面。</p>

<!-- 表格图片 -->
<img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/6-Table3-1.png" alt="Outputs generated by RLP and baselines for an example from AlpacaFarm" />

<p><strong>Instruction:</strong><br>What are different drawers I should have for clothes?<br><strong>指令：</strong><br>我应该为衣服准备哪些不同的抽屉？</p>

<p><strong>SFT Output:</strong><br>Different drawers for clothes can include a top drawer for socks and underwear, a middle drawer for shorts, pants, and skirts, a lower drawer for long-sleeved shirts, a separate drawer for t-shirts and tank tops, a drawer for sweaters and cardigans, a drawer for jackets, and a drawer for shoes.<br><strong>SFT 输出：</strong><br>衣服的不同抽屉可以包括：一个上层抽屉用来放袜子和内衣，一个中层抽屉用来放短裤、裤子和裙子，一个下层抽屉用来放长袖衬衫，一个单独的抽屉用来放T恤和背心，一个抽屉用来放毛衣和开衫，一个抽屉用来放夹克，以及一个抽屉用来放鞋子。</p>

<p><strong>PPO Output:</strong><br>Different drawers for clothes can include a master drawer for all items, such as pants, shirts, and jackets; a drawer for socks and underwear; a drawer for delicates, such as lingerie and fine clothing; a drawer for seasonal clothing, such as winter coats and swimwear; a drawer for work clothes, such as dress shirts and skirts; a drawer for shoes; and a drawer for accessories, such as belts, hats, and purses.<br><strong>PPO 输出：</strong><br>衣服的不同抽屉可以包括：一个主抽屉用来放所有物品，例如裤子、衬衫和夹克；一个抽屉用来放袜子和内衣；一个抽屉用来放内衣和精致衣物；一个抽屉用来放季节性衣物，如冬季外套和泳衣；一个抽屉用来放工作服，如衬衫和裙子；一个抽屉用来放鞋子；以及一个抽屉用来放配饰，如皮带、帽子和手袋。</p>

<p><strong>RLP-UML Output:</strong><br>Different drawers for clothes can include:<br>
- A top drawer for long-sleeved shirts and blouses.<br>
- A middle drawer for short-sleeved shirts and blouses.<br>
- A bottom drawer for sweaters, sweaters, and jackets.<br>
- A separate drawer for pants, shorts, and skirts.<br>
- A drawer for socks and underwear.<br>
- A drawer for accessories such as belts, scarves, and hats.<br>
- A separate drawer for seasonal clothing, such as winter coats and summer dresses.<br><strong>RLP-UML 输出：</strong><br>衣服的不同抽屉可以包括：<br>
- 上层抽屉用于长袖衬衫和衬衣。<br>
- 中层抽屉用于短袖衬衫和衬衣。<br>
- 下层抽屉用于毛衣、毛衣和夹克。<br>
- 单独的抽屉用于裤子、短裤和裙子。<br>
- 一个抽屉用于袜子和内衣。<br>
- 一个抽屉用于配饰，如皮带、围巾和帽子。<br>
- 单独的抽屉用于季节性衣物，如冬季外套和夏季裙子。</p>

<p><strong>RLP-SPG Output:</strong><br>Different drawers for clothes can include:<br>
- A master drawer for folded clothing, such as shirts, pants, skirts, and shorts.<br>
- A drawer for seasonal items, like sweaters and jackets.<br>
- A drawer for delicate items, like lingerie and fine fabrics.<br>
- A drawer for formal wear, like suits and dresses.<br>
- A drawer for accessories, like belts, scarves, and jewelry.<br>
- A drawer for uniforms, like work clothes and school uniforms.<br>
- A drawer for out-of-season items, like swimwear and winter coats.<br>
- A drawer for shoes, organized by type and use.<br>
- A drawer for cleaning supplies, like a laundry bin or ironing board.<br>
- A drawer for miscellaneous items, like first-aid kits and sewing supplies.<br><strong>RLP-SPG 输出：</strong><br>衣服的不同抽屉可以包括：<br>
- 一个主抽屉用于折叠衣物，如衬衫、裤子、裙子和短裤。<br>
- 一个抽屉用于季节性物品，如毛衣和夹克。<br>
- 一个抽屉用于精致物品，如内衣和细致的面料。<br>
- 一个抽屉用于正式服装，如西装和礼服。<br>
- 一个抽屉用于配饰，如皮带、围巾和珠宝。<br>
- 一个抽屉用于制服，如工作服和校服。<br>
- 一个抽屉用于过季物品，如泳衣和冬季外套。<br>
- 一个抽屉用于鞋子，按类型和用途整理。<br>
- 一个抽屉用于清洁用品，如洗衣篮或熨烫板。<br>
- 一个抽屉用于杂项物品，如急救包和缝纫用品。</p>

<p class="english">The difference between RLP and baselines can be observed qualitatively as well. For example, the case shown in Table 3 makes it sufficiently clear why RLP is so strongly preferred over our baselines from AlpacaFarm. Compared to RLP-UML, RLP-SPG generates even longer and more comprehensive outputs.</p>

<p class="chinese">RLP 与基准方法之间的差异也可以通过定性观察得到。例如，表 3 中的案例清晰地展示了为什么 RLP 比我们的基准方法在 AlpacaFarm 中更受青睐。与 RLP-UML 相比，RLP-SPG 生成的输出更长且更全面。</p>


<p><strong>5.6 Ablation Studies</strong></p>
<p><strong>5.6 消融研究</strong></p>

<p>This section provides comprehensive ablation studies to understand the efficacy of RLP.</p>
<p>本节提供了全面的消融研究，以理解 RLP 的有效性。</p>

<p>For consistency, all ablations are conducted using metric simulated win-rate that is computed against reference model text-davinci-003.</p>
<p>为了保持一致性，所有消融实验都使用相对于参考模型 <i>text-davinci-003</i> 计算的模拟获胜率。</p>



<p>Table 4: Ablation study on the representation learning loss for RLP-UML</p>
<p>表 4：关于 RLP-UML 表示学习损失的消融研究</p>
<p><img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/7-Table4-1.png" alt="Ablation study on the representation learning loss for RLP-UML"></p>

<p>Table 5: Ablation on different synthetic preference generation approaches.</p>
<p>表 5：不同合成偏好生成方法的消融研究</p>
<p><img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/7-Table5-1.png" alt="Ablation on different synthetic preference generation approaches"></p>

<p><strong>Information Bottleneck Loss</strong></p>
<p><strong>信息瓶颈损失</strong></p>

<p>We demonstrate the effectiveness of our multi-view information bottleneck loss \( L_M \) by replacing \( L_M \) in Eq. 1 with other alternatives of representation learning:</p>
<p>我们通过将公式 1 中的 \( L_M \) 替换为其他表示学习的替代方法，展示了多视角信息瓶颈损失的有效性：</p>

<ul>
  <li><i>InfoMax</i> (Poole et al., 2019) maximizes the mutual information between an input \( v(x) \) and its representation \( z \), i.e., \( I(v; z) \);</li>
  <li><i>InfoMax</i>（Poole 等，2019）最大化输入 \( v(x) \) 和其表示 \( z \) 之间的互信息，即 \( I(v; z) \)；</li>
  <li><i>MVI</i> (Bachman et al., 2019) is similar to InfoMax except that it maximizes the mutual information between its two views \( I(z_1; z_2) \);</li>
  <li><i>MVI</i>（Bachman 等，2019）与 InfoMax 类似，唯一不同的是它最大化两个视图 \( I(z_1; z_2) \) 之间的互信息；</li>
  <li><i>CL</i> (Caron et al., 2020) uses a contrastive learning loss. Positive pairs in this variant are obtained using our multi-view construction approach.</li>
  <li><i>CL</i>（Caron 等，2020）使用对比学习损失。此变体中的正对是通过我们的多视角构造方法获得的。</li>
</ul>


<p>Results in Table 4 show that the information bottleneck loss used in RLP-UML performs better than all other variants.</p>
<p>表 4 中的结果表明，RLP-UML 中使用的信息瓶颈损失比所有其他变体表现更好。</p>

<p>We also want to highlight that the approach of explicitly removing superficial information in RLP-UML makes it outperform InfoMax and MVI by 4.7% and 1.0% in AlpacaFarm, and 2.0% and 1.0% in LLMBar, respectively.</p>
<p>我们还想强调，RLP-UML 明确去除表面信息的方法使其在 AlpacaFarm 中比 InfoMax 和 MVI 提高了 4.7% 和 1.0%，在 LLMBar 中分别提高了 2.0% 和 1.0%。</p>

<p>This validates our claim that the multi-view information bottleneck loss used in RLP-UML effectively removes the superficial information.</p>
<p>这验证了我们的观点，即 RLP-UML 中使用的多视角信息瓶颈损失能够有效去除表面信息。</p>

<p><strong>Synthetic Preference Generation</strong></p>
<p><strong>合成偏好生成</strong></p>

<p>We compare RLP with two types of synthetic preference generation approaches:</p>
<p>我们将 RLP 与两种类型的合成偏好生成方法进行比较：</p>

<ul>
  <li>I. sampling a pair of responses for each instruction and then labeling its preference with LLMs;</li>
  <li>Ⅰ. 为每个指令采样一对响应，然后使用 LLM 对其偏好进行标注；</li>

  <li>II. sampling a set of outputs for each instruction and then selecting a preferred and a nonpreferred one to construct pairwise preference data.</li>
  <li>Ⅱ. 为每个指令采样一组输出，然后选择一个偏好和一个非偏好的输出，构建成对的偏好数据。</li>
</ul>

<p>For type I, we implement RLAIF (Lee et al., 2023) that labels preferences with policy \( \pi_\theta \), and Reward that ranks the two outputs with reward model \( r_\phi \) and assumes the top ranked output as the preferred one.</p>
<p>对于类型 I，我们实现了 RLAIF（Lee 等，2023），它使用策略 \( \pi_\theta \) 标注偏好，并且使用奖励模型 \( r_\phi \) 对两个输出进行排名，将排名靠前的输出视为首选。</p>

<p>For type II, we study a variant of RLP, SelectAll, that sets \( \gamma = 0 \) for selective generation and no longer rejects low confidence data.</p>
<p>对于类型 II，我们研究了 RLP 的一个变体 SelectAll，它将 \( \gamma = 0 \) 设置为选择性生成，并且不再拒绝低置信度数据。</p>

<p>Table 5 shows the accuracy of generated preference data and the win-rate of the corresponding LLMs. Golden preferences are labeled with AlpacaFarm simulated annotator.</p>
<p>表 5 显示了生成的偏好数据的准确性以及相应 LLM 的胜率。黄金偏好由 AlpacaFarm 模拟注释器标注。</p>

<p>These results indicate that RLP-SPG outperforms all ablation variants in terms of synthetic preference quality and LLM performance.</p>
<p>这些结果表明，RLP-SPG 在合成偏好质量和 LLM 性能方面优于所有消融变体。</p>

<p>We can also observe that: 1. Sampling a set of outputs rather than a pair for each instruction helps encourage output diversity and leads to high-quality preference generation.</p>
<p>我们还可以观察到：1. 对每个指令采样一组输出而不是一对输出有助于鼓励输出的多样性，并促进高质量的偏好生成。</p>

<p>2. The confidence score based on multiple sampling can be used for selective generation and further improve preference quality.</p>
<p>2. 基于多次采样的置信度得分可用于选择性生成，并进一步提高偏好质量。</p>

<p>3. LLMs trained with more accurate preference data generally perform better and obtain higher win-rate scores.</p>
<p>3. 使用更准确的偏好数据训练的 LLM 通常表现更好，并获得更高的胜率。</p>



<p>Table 6: An example instruction and corresponding simulated preference data from RLAIF and RLP-SPG</p>
<p>表 6：来自 RLAIF 和 RLP-SPG 的示例指令及相应的模拟偏好数据</p>
<p><img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/8-Table6-1.png" alt="An example instruction and corresponding simulated preference data from RLAIF and RLP-SPG"></p>



<p><strong>Instruction:</strong><br>Generate a definition for the term “artificial intelligence”.<br><strong>指令：</strong><br>为“人工智能”一词生成定义。</p>

<p><strong>RLAIF Simulated Preference Data:</strong><br>
<strong>\( y_w \):</strong> Artificial intelligence (AI) is a branch of computer science that enables machines to learn from experience, adjust to new input, and mimic human decision-making processes. AI technology is used to develop systems that can interact with their environment and achieve tasks such as robotic navigation, natural language processing, and computer vision.<br>
<strong>\( y_w \):</strong> 人工智能（AI）是计算机科学的一个分支，使机器能够通过经验学习、适应新输入，并模仿人类的决策过程。人工智能技术用于开发能够与环境互动并执行任务的系统，如机器人导航、自然语言处理和计算机视觉。<br>

<strong>\( y_l \):</strong> Artificial intelligence (AI) is a field of study focused on developing computers and machines that are capable of performing tasks that are typically done by humans, such as reasoning, problem solving, and decision-making. AI technology is used in a variety of applications, including robotics, games, speech recognition, natural language processing, and automation.<br>
<strong>\( y_l \):</strong> 人工智能（AI）是一个研究领域，致力于开发能够执行通常由人类完成的任务的计算机和机器，例如推理、问题解决和决策。人工智能技术应用于多种领域，包括机器人技术、游戏、语音识别、自然语言处理和自动化。</p>

<p><strong>RLP Simulated Preference Data:</strong><br>
<strong>\( y_w \):</strong> Artificial intelligence (AI) is a branch of computer science that involves the use of sophisticated algorithms and technology to enable machines to think, learn, and act like humans. AI systems can be programmed to perform a variety of tasks, from simple to complex, such as recognizing speech, understanding natural language, automating tasks, and more. AI technology is used in a wide range of applications, from self-driving cars to virtual assistants to automated customer service. In short, AI is the science and technology of making computers and machines smart and capable of doing things traditionally done by humans.<br>
<strong>\( y_w \):</strong> 人工智能（AI）是计算机科学的一个分支，涉及使用复杂的算法和技术使机器能够像人类一样思考、学习和行动。人工智能系统可以编程执行各种任务，从简单到复杂，如语音识别、理解自然语言、自动化任务等。人工智能技术应用于广泛的领域，从自动驾驶汽车到虚拟助手再到自动化客户服务。简而言之，人工智能是使计算机和机器变得智能并能够执行传统上由人类完成的任务的科学与技术。<br>

<strong>\( y_l \):</strong> Artificial intelligence (AI) is a field of study focused on developing computers and machines that can think and act like humans. AI technology allows machines to interact with their environment and to learn from their mistakes, just like humans do.<br>
<strong>\( y_l \):</strong> 人工智能（AI）是一个研究领域，致力于开发能够像人类一样思考和行动的计算机和机器。人工智能技术使机器能够与环境互动，并像人类一样从错误中学习。</p>



<p>Table 7: Cases of rejected and accepted instructions for selective synthetic preference generation by RLP-SPG</p>
<p>表 7：RLP-SPG 在选择性合成偏好生成中被拒绝和接受的指令案例</p>
<p><img src="https://figures.semanticscholar.org/969c1dc38c98f2dbfb981a542880559890366494/8-Table7-1.png" alt="Cases of rejected and accepted instructions for selective synthetic preference generation by RLP-SPG"></p>

<p>
    <strong>Reject:</strong> 描述查理二世的生平与统治。 0.1<br>
    <strong>Reject:</strong> 什么类型的水果适合加入水果沙拉？ 0.2<br>
    <strong>Reject:</strong> 研究一位著名科学家并提供简短的传记。 0.2<br>
    <strong>Reject:</strong> 用五个词句描述你的一天。 0.4<br>
    <strong>Accept:</strong> 写一篇科学解释为什么天空是蓝色的。 0.6<br>
    <strong>Accept:</strong> 查找以下词汇的同义词：‘Tenacious’（坚韧的）。 0.7<br>
    <strong>Accept:</strong> 找出以下段落的主题。 0.8<br>
    <strong>Accept:</strong> 创建一个推文，用140个字符以内总结以下新闻文章。 0.9
    </p>
    

<p>5.7 Further Analysis</p>
<p>这里我们展示了 LLM 训练过程中的中间结果的进一步分析。表 6 显示了 RLAIF 和 RLP-SPG 模拟偏好数据的一个例子。在这个例子中，RLAIF 的两个输出（\( y_w \) 和\( y_l \)）看起来相似。然而，RLAIF 偏好 \( y_w \)，这可能在训练过程中引入噪声。相反，RLP-SPG 的 \( y_w \) 比 RLP-SPG 的\( y_l \) 更为全面，从而生成更准确的标签。</p>

<p>We also find a major difference in the length distributions of RLP-SPG outputs, with preferred outputs \( y_w \) (510 characters on average) significantly longer than non-preferred outputs\( y_l \) (449 characters on average).</p>
<p>我们还发现 RLP-SPG 输出的长度分布存在显著差异，首选输出 \( y_w \)（平均 510 个字符）明显长于非首选输出\( y_l \)（平均 449 个字符）。</p>

<p>Table 7 also demonstrates cases of rejected and accepted instructions for selective synthetic preference generation by RLP-SPG (rejecting low confidence generations). It can be observed that open-ended instructions (e.g., more subjective and creative) tend to have low confidences and be rejected.</p>
<p>表 7 还展示了 RLP-SPG 在选择性合成偏好生成中被拒绝和接受的指令的案例（拒绝低置信度生成）。可以观察到，开放式指令（例如，更主观和创造性的）往往具有较低的置信度，并且被拒绝。</p>

<p>6 Conclusion</p>
<p>6 结论</p>

<p>In this paper, we propose reward learning on policy (RLP), a novel framework to align LLMs with human preferences. RLP learns robust representations of the policy’s data distribution via optimizing a multi-view information bottleneck loss. RLP also simulates preferences with a set of policy outputs, which enables confidence estimation and selective generation. Extensive experiments demonstrate that RLP outperforms SOTA baselines.</p>
<p>在本文中，我们提出了奖励学习策略（RLP），一个新颖的框架，用于将 LLM 与人类偏好对齐。RLP 通过优化多视图信息瓶颈损失来学习策略数据分布的鲁棒表示。RLP 还通过一组策略输出来模拟偏好，从而实现置信度估计和选择性生成。大量实验表明，RLP 超越了现有最先进的基准。</p>

<p>Limitations</p>
<p>局限性</p>

<p>While we have carefully studied the effectiveness of RLP compared to several baselines on three benchmark datasets for LLaMA-7B, we have not yet empirically verified our conclusions when aligning larger pretrained LLMs. It would also be interesting to align new SOTA pretrained LLMs such as LLaMA 2 (Touvron et al., 2023b) and test other methods for fitting preference data like DPO (Rafailov et al., 2024).</p>
<p>虽然我们已经仔细研究了 RLP 在与多个基准进行比较时的有效性，特别是在三个 LLaMA-7B 的基准数据集上，但我们尚未在对齐更大的预训练 LLM 时实证验证我们的结论。将新一代的预训练 LLaMA 2（Touvron 等，2023b）进行对齐，并测试其他适应偏好数据的方法（如 DPO（Rafailov 等，2024））也将是一个有趣的方向。</p>

<p>Meanwhile, all of our training and evaluation data are in English, and we have not tested in other languages. Performance may degenerate especially in low-resource languages when pretrained LLMs have not been trained with these data.</p>
<p>同时，我们的所有训练和评估数据都为英文，且未在其他语言中进行测试。当预训练 LLM 未在这些语言数据上进行训练时，尤其是在资源匮乏的语言中，性能可能会退化。</p>

<p>Ethics Statement</p>
<p>伦理声明</p>

<p>This work does not raise any direct ethical issues. In the proposed work, we seek to develop a novel RLHF framework to align large language models (LLMs) with human preferences. Concretely, we propose to learn reward models on policy to keep it on-distribution. We believe this work can benefit the field of LLMs, with the potential to benefit other fields requiring NLP models. All experiments are conducted on open datasets.</p>
<p>这项工作没有引发任何直接的伦理问题。在本研究中，我们旨在开发一个新颖的 RLHF 框架，将大型语言模型（LLMs）与人类偏好对齐。具体来说，我们提议在策略上学习奖励模型以保持其分布一致。我们相信，这项工作可以造福 LLM 领域，并有潜力惠及其他需要 NLP 模型的领域。所有实验均在开放数据集上进行。</p>










    </div>

</body>
</html>
